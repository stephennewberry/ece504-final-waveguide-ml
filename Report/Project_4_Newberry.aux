\relax 
\citation{newberry_machine_2022}
\citation{newberry_machine_2022-1}
\citation{noauthor_exponential_2022}
\citation{noauthor_numpyrandomexponential_nodate}
\citation{yates_probability_2014}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\newlabel{intro}{{I}{1}}
\newlabel{eq:exp_scale}{{1}{1}}
\newlabel{eq:exp_pdf}{{2}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Probability density function plotted for the exponential distribution with various rate parameters.}}{1}{}\protected@file@percent }
\newlabel{fig:exp_pdf}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Cumulative distribution function plotted for the exponential distribution with various rate parameters.}}{1}{}\protected@file@percent }
\newlabel{fig:exp_cdf}{{2}{1}}
\citation{newberry_machine_2022-1}
\citation{newberry_machine_2022-1}
\citation{newberry_machine_2022-1}
\citation{newberry_machine_2022}
\citation{noauthor_numpyrandomexponential_nodate}
\citation{newberry_machine_2022}
\citation{pozar_microwave_2012}
\citation{pozar_microwave_2012}
\citation{pozar_microwave_2012}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Neural net model prediction performance with various levels of exponential noise applied. This model was trained with noiseless data.}}{2}{}\protected@file@percent }
\newlabel{fig:predictions_nn_model_noiseless_training}{{3}{2}}
\newlabel{eq:exp_scale}{{3}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Formulation}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Training and Test Data}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Decision tree model prediction performance with various levels of exponential noise applied. This model was trained with noiseless data.}}{2}{}\protected@file@percent }
\newlabel{fig:predictions_dt_model_noiseless_training}{{4}{2}}
\newlabel{eq:cutoff}{{4}{2}}
\citation{newberry_machine_2022-1}
\citation{noauthor_random_nodate}
\citation{newberry_machine_2022-1}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Configuration of training and test data fields.}}{3}{}\protected@file@percent }
\newlabel{tbl:data_prep}{{I}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}1}TE Mode Equations}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}2}TM Mode Equations}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Noise Specification}{3}{}\protected@file@percent }
\citation{newberry_machine_2022-1}
\citation{newberry_machine_2022-1}
\citation{newberry_machine_2022-1}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Selected Scale Parameter Values for Evaluation.}}{4}{}\protected@file@percent }
\newlabel{tbl:noise_spec}{{II}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of all data after further noise is applied.}}{4}{}\protected@file@percent }
\newlabel{fig:noise_comparison}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Deep Neural Net}{4}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Comparison of Overall Loss Values after Neural Net Training.}}{4}{}\protected@file@percent }
\newlabel{tbl:loss_compare}{{III}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Comparison of training and validation loss after fitting model.}}{4}{}\protected@file@percent }
\newlabel{tbl:loss}{{IV}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Plot of all loss parameters during the training phase.}}{4}{}\protected@file@percent }
\newlabel{fig:training_loss_plot}{{6}{4}}
\citation{newberry_machine_2022-1}
\citation{geron_hands-machine_2019}
\citation{pedregosa_scikit-learn_2011}
\citation{noauthor_110_nodate}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Flow chart of the Keras deep neural net model used within this work.}}{5}{}\protected@file@percent }
\newlabel{fig:model_diagram}{{7}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-D}}Decision Tree}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces First three layers of the implemented decision tree for the $M$ output feature.}}{6}{}\protected@file@percent }
\newlabel{fig:dt_top}{{8}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Evaluated Data Scores Based on Standard Deviation of Noise.}}{6}{}\protected@file@percent }
\newlabel{tbl:scores}{{V}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Deep Neural Net}{6}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Error Count for Evaluation Data from Neural Net}}{6}{}\protected@file@percent }
\newlabel{tbl:error_count}{{VI}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Count of errors across all noise configurations for neural net.}}{6}{}\protected@file@percent }
\newlabel{fig:error_count}{{9}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Decision Tree}{6}{}\protected@file@percent }
\citation{newberry_machine_2022-1}
\citation{newberry_machine_2022-1}
\citation{yates_probability_2014}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Classification Errors in Decision Tree Algorithm from 5000-Sample Test Dataset.}}{7}{}\protected@file@percent }
\newlabel{tbl:dt_test_class_res}{{VII}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Histogram of the count of percent error occurrences for the decision tree model on the frequency output.}}{7}{}\protected@file@percent }
\newlabel{fig:dt_test_dataset_freq_perc_error_hist}{{10}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Error Count for Evaluation Data from Decision Tree}}{7}{}\protected@file@percent }
\newlabel{tbl:error_count_dt}{{VIII}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Count of errors across all noise configurations for the decision trees.}}{7}{}\protected@file@percent }
\newlabel{fig:error_count_dt}{{11}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Using Previously-Trained Model}{7}{}\protected@file@percent }
\citation{newberry_machine_2022-1}
\citation{newberry_machine_2022-1}
\bibstyle{IEEEtran}
\bibdata{reference}
\bibcite{newberry_machine_2022}{1}
\bibcite{newberry_machine_2022-1}{2}
\bibcite{noauthor_exponential_2022}{3}
\bibcite{noauthor_numpyrandomexponential_nodate}{4}
\bibcite{yates_probability_2014}{5}
\bibcite{pozar_microwave_2012}{6}
\bibcite{noauthor_random_nodate}{7}
\bibcite{geron_hands-machine_2019}{8}
\bibcite{pedregosa_scikit-learn_2011}{9}
\bibcite{noauthor_110_nodate}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Count of errors across all noise configurations for the exponential noise data ran through the neural net model from the previous project.}}{8}{}\protected@file@percent }
\newlabel{fig:error_count_nn_prior_model}{{12}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Count of errors across all noise configurations for the exponential noise data ran through the decision tree models from the previous project.}}{8}{}\protected@file@percent }
\newlabel{fig:error_count_dt_prior_model}{{13}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Discussion}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{8}{}\protected@file@percent }
\gdef \@abspage@last{8}

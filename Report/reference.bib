
@book{pozar_microwave_2012,
	address = {Hoboken, NJ},
	edition = {4th ed},
	title = {Microwave engineering},
	isbn = {978-0-470-63155-3},
	publisher = {Wiley},
	author = {Pozar, David M.},
	year = {2012},
	keywords = {Microwave circuits, Microwave devices, Microwaves},
}

@misc{noauthor_adam_nodate,
	title = {Adam},
	url = {https://keras.io/api/optimizers/adam/},
	urldate = {2021-12-29},
	file = {Adam:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\QV25F2F7\\adam.html:text/html},
}

@misc{noauthor_keras_nodate,
	title = {Keras: the {Python} deep learning {API}},
	url = {https://keras.io/},
	urldate = {2021-12-29},
	file = {Keras\: the Python deep learning API:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\8NYFWWLR\\keras.io.html:text/html},
}

@misc{team_keras_nodate,
	title = {Keras documentation: {Losses}},
	shorttitle = {Keras documentation},
	url = {https://keras.io/api/losses/},
	abstract = {Keras documentation},
	language = {en},
	urldate = {2022-01-10},
	author = {Team, Keras},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\BI7R9XLN\\losses.html:text/html},
}

@misc{noauthor_root-mean-square_2021,
	title = {Root-mean-square deviation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/wiki/RMSD},
	abstract = {The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.RMSD is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSD is better than a higher one. However, comparisons across different types of data would be invalid because the measure is dependent on the scale of the numbers used.
RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers.},
	language = {en},
	urldate = {2022-01-14},
	journal = {Wikipedia},
	month = aug,
	year = {2021},
	note = {Page Version ID: 1037360077},
}

@book{geron_hands-machine_2019,
	address = {Beijing [China] ; Sebastopol, CA},
	edition = {Second edition},
	title = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4920-3264-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
	publisher = {O'Reilly Media, Inc},
	author = {Géron, Aurélien},
	year = {2019},
	keywords = {Artificial intelligence, Machine learning, Python (Computer program language), TensorFlow},
	annote = {"2nd edition updated for TensorFlow 2"--Cover},
}

@misc{zadehgol_ece_2022,
	address = {Moscow, ID},
	type = {University {Course}},
	title = {{ECE} 504, {University} of {Idaho}},
	author = {Zadehgol, Ata},
	year = {2022},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830},
}

@misc{noauthor_cross-validation_2022,
	title = {Cross-validation (statistics)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/wiki/Cross-validation_(statistics)},
	abstract = {Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.
Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).
One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.
In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.},
	language = {en},
	urldate = {2022-02-08},
	journal = {Wikipedia},
	month = jan,
	year = {2022},
	note = {Page Version ID: 1064674372},
}

@misc{noauthor_mnist_2022,
	title = {{MNIST} database},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=MNIST_database&oldid=1071024311},
	abstract = {The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by "re-mixing" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support-vector machine to get an error rate of 0.8\%. An extended dataset similar to MNIST called EMNIST has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits and characters.},
	language = {en},
	urldate = {2022-02-10},
	journal = {Wikipedia},
	month = feb,
	year = {2022},
	note = {Page Version ID: 1071024311},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\WZXZMBBR\\MNIST_database.html:text/html},
}

@misc{smith_us_nodate,
	title = {The {U}.{S}. {Cities} {Where} {People} {Earn} {The} {Biggest} {And} {Smallest} {Paychecks}},
	url = {https://www.forbes.com/sites/jacquelynsmith/2013/11/27/the-u-s-cities-where-people-earn-the-biggest-and-smallest-paychecks/},
	abstract = {These are the cities where employees earn the largest and smallest paychecks, according to data from PayScale.com.},
	language = {en},
	urldate = {2022-02-10},
	journal = {Forbes},
	author = {Smith, Jacquelyn},
	note = {Section: Leadership},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\UPXBCF47\\the-u-s-cities-where-people-earn-the-biggest-and-smallest-paychecks.html:text/html},
}

@misc{noauthor_sklearnensemblegradientboostingregressor_nodate,
	title = {sklearn.ensemble.{GradientBoostingRegressor}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html},
	abstract = {Examples using sklearn.ensemble.GradientBoostingRegressor: Plot individual and voting regression predictions Plot individual and voting regression predictions, Gradient Boosting regression Gradient...},
	language = {en},
	urldate = {2022-02-11},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\H9ZJIZMW\\sklearn.ensemble.GradientBoostingRegressor.html:text/html},
}

@misc{noauthor_sklearnensemblehistgradientboostingregressor_nodate,
	title = {sklearn.ensemble.{HistGradientBoostingRegressor}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html},
	abstract = {Examples using sklearn.ensemble.HistGradientBoostingRegressor: Release Highlights for scikit-learn 1.0 Release Highlights for scikit-learn 1.0, Release Highlights for scikit-learn 0.23 Release High...},
	language = {en},
	urldate = {2022-02-11},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\NM5EJXAY\\sklearn.ensemble.HistGradientBoostingRegressor.html:text/html},
}

@misc{noauthor_sklearnlinear_modellinearregression_nodate,
	title = {sklearn.linear\_model.{LinearRegression}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.linear_model.LinearRegression.html},
	abstract = {Examples using sklearn.linear\_model.LinearRegression: Principal Component Regression vs Partial Least Squares Regression Principal Component Regression vs Partial Least Squares Regression, Plot ind...},
	language = {en},
	urldate = {2022-02-11},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\4GI53UXJ\\sklearn.linear_model.LinearRegression.html:text/html},
}

@misc{noauthor_sklearnlinear_modelsgdclassifier_nodate,
	title = {sklearn.linear\_model.{SGDClassifier}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.linear_model.SGDClassifier.html},
	abstract = {Examples using sklearn.linear\_model.SGDClassifier: Model Complexity Influence Model Complexity Influence, Out-of-core classification of text documents Out-of-core classification of text documents, ...},
	language = {en},
	urldate = {2022-02-11},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\9LA97BAH\\sklearn.linear_model.SGDClassifier.html:text/html},
}

@misc{noauthor_sklearnsvmsvc_nodate,
	title = {sklearn.svm.{SVC}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.svm.SVC.html},
	abstract = {Examples using sklearn.svm.SVC: Release Highlights for scikit-learn 0.24 Release Highlights for scikit-learn 0.24, Release Highlights for scikit-learn 0.22 Release Highlights for scikit-learn 0.22,...},
	language = {en},
	urldate = {2022-02-11},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\P3PSEY5P\\sklearn.svm.SVC.html:text/html},
}

@article{kumar_knowledge-based_2022,
	title = {Knowledge-{Based} {Neural} {Networks} for {Fast} {Design} {Space} {Exploration} of {Hybrid} {Copper}-{Graphene} {On}-{Chip} {Interconnect} {Networks}},
	volume = {64},
	issn = {1558-187X},
	doi = {10.1109/TEMC.2021.3091714},
	abstract = {In this article, an artificial neural network (ANN) is developed in order to predict the per-unit-length (p. u. l.) parameters of hybrid copper-graphene on-chip interconnects from a prior knowledge of their structural geometry and layout. The salient feature of the proposed ANN is that it combines knowledge of the p. u. l. parameters extracted from empirical models along with that extracted from a rigorous full-wave electromagnetic solver. As a result, the proposed ANN is referred to as a knowledge-based neural network (KBNN). The KBNN has been found to converge to the same accuracy as a conventional ANN but at the expense of far smaller training time costs. As a result, the KBNN is much more suitable for performing design space explorations.},
	number = {1},
	journal = {IEEE Transactions on Electromagnetic Compatibility},
	author = {Kumar, Rahul and Narayan, S. S. Likith and Kumar, Somesh and Roy, Sourajeet and Kaushik, Brajesh Kumar and Achar, Ramachandra and Sharma, Rohit},
	month = feb,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Electromagnetic Compatibility},
	keywords = {Computational modeling, Mathematical model, Artificial neural networks (ANNs), Copper, design space explorations, Knowledge based systems, knowledge-based neural networks (KBNNs), Neurons, on-chip interconnects, per-unit-length (p. u. l.) parameters, Space exploration, Training, transient response},
	pages = {182--195},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\JSAR4BEZ\\9492285.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\BLXUYKLV\\Kumar et al. - 2022 - Knowledge-Based Neural Networks for Fast Design Sp.pdf:application/pdf},
}

@misc{team_pandas-devpandas_2020,
	title = {pandas-dev/pandas: {Pandas}},
	url = {https://doi.org/10.5281/zenodo.3509134},
	publisher = {Zenodo},
	author = {team, The pandas development},
	month = feb,
	year = {2020},
	doi = {10.5281/zenodo.3509134},
}

@misc{noauthor_artificial_2022,
	title = {Artificial neural network},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_neural_network&oldid=1075364983},
	abstract = {Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems inspired by the biological neural networks that constitute animal brains.

An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives a signal then processes it and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.},
	language = {en},
	urldate = {2022-03-11},
	journal = {Wikipedia},
	month = mar,
	year = {2022},
	note = {Page Version ID: 1075364983},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\LHPU8N76\\Artificial_neural_network.html:text/html},
}

@misc{noauthor_rectifier_2022,
	title = {Rectifier (neural networks)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Rectifier_(neural_networks)&oldid=1074306665},
	abstract = {In the context of artificial neural networks, the rectifier or ReLU (Rectified Linear Unit) activation function is an activation function defined as the positive part of its argument:

  
    
      
        f
        (
        x
        )
        =
        
          x
          
            +
          
        
        =
        max
        (
        0
        ,
        x
        )
      
    
    \{{\textbackslash}displaystyle f(x)=x{\textasciicircum}\{+\}={\textbackslash}max(0,x)\}
  where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering.
This activation function started showing up in the context of visual feature extraction in hierarchical neural networks starting in the late 1960s. It was later argued that it  has  strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience.},
	language = {en},
	urldate = {2022-03-11},
	journal = {Wikipedia},
	month = feb,
	year = {2022},
	note = {Page Version ID: 1074306665},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\FHINVNPC\\Rectifier_(neural_networks).html:text/html},
}

@misc{noauthor_home_nodate,
	title = {Home {Page} {\textbar} {Signal} {Integrity} {Journal}},
	url = {https://www.signalintegrityjournal.com/},
	urldate = {2022-03-31},
	file = {Home Page | Signal Integrity Journal:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\IKG9F6IY\\www.signalintegrityjournal.com.html:text/html},
}

@misc{noauthor_home_nodate-1,
	title = {Home {Page} {\textbar} {Signal} {Integrity} {Journal}},
	url = {https://www.signalintegrityjournal.com/},
	urldate = {2022-03-31},
}

@misc{noauthor_home_nodate-2,
	title = {Home {Page} {\textbar} {Signal} {Integrity} {Journal}},
	url = {https://www.signalintegrityjournal.com/},
	urldate = {2022-03-31},
}

@techreport{newberry_machine_2022,
	title = {Machine {Learning} for {Electromagnetics}: {Project} 2},
	abstract = {Machine learning has the fascinating ability to
extract patterns and information out of massive and complex data
sets. In this paper, a deep neural net is constructed which extracts
waveguide parameters from field data and geometry.This neural
net is flexible and works with arbitrary waveguide geometry,
using any width between 0.1 cm to 10 cm and any height between
0.05 cm and 5 cm. Using the field data and geometry, the neural
net is able to determine the modal configuration (TE or TM),
the field component (E- or H-field, x-, y- or z- direction) as well
as the m and n of the propagation mode.
The model was trained with only 20,000 training data points
and 4,000 validation data points. The entire model takes only
about 30 minutes to train, thus it is not necessarily suitable
for online learning but is reasonable for use in batch learning.
The accuracy of the neural net is adequate at predicting modal
parameters, mode and field components, with an error value
ranging from about 9.6\% to 16.3\% on those particular outputs
from an evaluation data set of 1,000 points.},
	language = {English},
	institution = {University of Idaho},
	author = {Newberry, Stephen},
	month = mar,
	year = {2022},
	pages = {8},
}

@misc{noauthor_pickle_nodate,
	title = {pickle — {Python} object serialization — {Python} 3.10.4 documentation},
	url = {https://docs.python.org/3/library/pickle.html},
	urldate = {2022-04-10},
	file = {pickle — Python object serialization — Python 3.10.4 documentation:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\CEHQRB2G\\pickle.html:text/html},
}

@misc{noauthor_random_nodate,
	title = {Random sampling (numpy.random) — {NumPy} v1.22 {Manual}},
	url = {https://numpy.org/doc/stable/reference/random/},
	urldate = {2022-04-10},
	file = {Random sampling (numpy.random) — NumPy v1.22 Manual:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\MIGZSDW2\\random.html:text/html},
}

@misc{noauthor_110_nodate,
	title = {1.10. {Decision} {Trees}},
	url = {https://scikit-learn/stable/modules/tree.html},
	abstract = {Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning s...},
	language = {en},
	urldate = {2022-04-12},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\AY9J2X3K\\tree.html:text/html},
}

@misc{noauthor_14_nodate,
	title = {1.4. {Support} {Vector} {Machines}},
	url = {https://scikit-learn/stable/modules/svm.html},
	abstract = {Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high ...},
	language = {en},
	urldate = {2022-04-12},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\EYU7APVB\\svm.html:text/html},
}

@techreport{newberry_machine_2022-1,
	title = {Machine {Learning} for {Electromagnetics}: {Project} 3},
	abstract = {Using a machine learning model with perfect data
is an exercise which may be valuable in a classroom setting but
is unlikely to be encountered in reality. This project extends
the previous work looking at cross-sectional field data in a
rectangular waveguide and adds various amounts of noise to the
data. This represents a more realistic scenario in which the field
data is obtained by either simulation or measurement rather than
closed-form analytical solutions. Although the measurement of
actual field data within a waveguide is unlikely, the techniques are
applicable to other types of radio frequency (RF) measurement
such as that used in an anechoic chamber for electromagnetic
interference or compatibility testing (EMI/EMC) as required by
the Federal Communications Commission (FCC). Additionally,
while the ideal simulation is perfect, there are always non-
idealities introduced by the meshing process or other aspects of
the computational electromagnetic algorithms. The processes in
this paper show performance which degrades from 0.4\% error
at best up to 88.3\% error at worst across the various noise
configurations applied when using a neural net. An alternate
decision tree model is then presented which can predict certain
classification outputs with zero error out of 1000 evaluation data
points with minimal noise. This decision tree then is shown to
have a worst-case classification error of 59.6\%, beating out the
complex neural net},
	language = {English},
	institution = {University of Idaho},
	author = {Newberry, Stephen},
	month = apr,
	year = {2022},
	pages = {8},
}

@misc{noauthor_exponential_2022,
	title = {Exponential distribution},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Exponential_distribution&oldid=1082827254},
	abstract = {In probability theory and statistics, the exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It is a particular case of the gamma distribution. It is the continuous analogue of the geometric distribution, and it has the key property of being memoryless. In addition to being used for the analysis of Poisson point processes it is found in various other contexts.
The exponential distribution is not the same as the class of exponential families of distributions, which is a large class of probability distributions that includes the exponential distribution as one of its members, but also includes the normal distribution, binomial distribution, gamma distribution, Poisson, and many others.},
	language = {en},
	urldate = {2022-05-07},
	journal = {Wikipedia},
	month = apr,
	year = {2022},
	note = {Page Version ID: 1082827254},
	file = {Snapshot:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\KD54GMGX\\Exponential_distribution.html:text/html},
}

@misc{noauthor_numpyrandomexponential_nodate,
	title = {numpy.random.exponential — {NumPy} v1.22 {Manual}},
	url = {https://numpy.org/doc/stable/reference/random/generated/numpy.random.exponential.html},
	urldate = {2022-05-07},
	file = {numpy.random.exponential — NumPy v1.22 Manual:C\:\\Users\\steph\\Documents\\Education\\Z998_CITATIONS\\storage\\8JMITI9R\\numpy.random.exponential.html:text/html},
}

@book{yates_probability_2014,
	address = {Hoboken, NJ},
	edition = {Third edition},
	title = {Probability and stochastic processes: a friendly introduction for electrical and computer engineers},
	isbn = {978-1-118-32456-1},
	shorttitle = {Probability and stochastic processes},
	abstract = {"In Probability and Stochastic Processes: A Friendly Introduction for Electrical and Computer Engineers, readers are able to grasp the concepts of probability and stochastic processes, and apply these in professional engineering practice. The 3rd edition also includes quiz solutions within the appendix of the text. The resource presents concepts clearly as a sequence of building blocks identified as an axiom, definition or theorem. This approach allows for a better understanding of the material, which can be utilized in solving practical problems"--},
	publisher = {John Wiley \& Sons},
	author = {Yates, Roy D. and Goodman, David J.},
	year = {2014},
	keywords = {MATHEMATICS / Probability \& Statistics / General, Probabilities, Stochastic processes},
	annote = {Machine generated contents note: Chapter 1. Experiments, Models, and Probabilities Chapter 2. Discrete Random Variables Chapter 3. Continuous Random Variables Chapter 4. Pairs of Random Variables Chapter 5. Random Vectors Chapter 6. Sums of Random Variables Chapter 7. Parameter Estimation Using the Sample Mean Chapter 8. Hypothesis Testing Chapter 9. Estimation of a Random Variable Chapter 10. Stochastic Processes Chapter 11. Random Signal Processing Chapter 12. Markov Chains},
}
